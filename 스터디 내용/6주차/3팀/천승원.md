파일 크롤링 
========
[1]. urllib라이브러리 활용한 웹페이지 크롤링.
--------
## import urllib.request - 파일을 읽고 저장하는 2가지 방법.
--------
### 1. urllib.request.urlopen(url)을 사용하는 방법
- 파일을 읽어온 뒤 새로운 파일을 생성하여 복붙하는 방식이라 생각하면 될것같다.
--------
```
- 1. url = 읽을 url, savename = 파일 이름.
- 2. result = urllib.request.urlopen(url)
- 3. data = result.read()로 데이터 읽기, <바이너리 데이터>라는 것 주의.
- 4. f = open(savename, mode = “wb”) - 파일을 savename이라는 이름으로 오픈한다.
- 5. f.write(data)를 통해 읽어온 데이터를 작성한다.
- 6. f.close()를 통해 파일 종료
```
--------
추가로 조사한 내용 -> 
1. with 함수를 이용하면 close 없이도 파일을 자동으로 닫을 수 있다.
```
with 파일 오픈 as 파일의 이름:   
	  파일의 이름.write(data)
```
2. open(savename, mode = “wb”)
read데이타로 읽어들인 것은 바이너리 데이터이므로 w가 아닌 wb로 받아주는 것.
텍스트 데이터는 wt를 사용하며, 사실 이것은 기본 default값이므로 딱히 쓰지 않고 w로 사용하여도 된다. 
https://blockdmask.tistory.com/454
3. 그리고 open함수의 경우, 읽기(r), 쓰기(w), 추가(a)모드가 있으며, 쓰기(w)모드의 경우, 파일을 완전히 새로 만드는 것과 동일하다는 것을 기억해야한다. ‘r’읽기 모드는 사실 모두 익숙할테니 제외하고, ‘w’모드의 경우, 그 이름의 파일이 없을 경우 새로 생성하며, 이미 생성된 파일의 경우 그 파일을 완전히 초기화하고 새로운 내용을 적는다는 것에 유의하자. 원래 파일에 추가를 원할 경우 추가(a)모드를 사용해야 한다. 
더 자세한 내용, 총정리 : https://wikidocs.net/26

--------
### 2. urllib.request.urlretrieve(url, savename)를 사용하는 방법
--------
그냥 한번에 savename에 url에서 다운로드한 내용을 저장한다.
- 1. url = 읽을 url, savename = 파일 이름.
- 2. urllib.request.urlretrieve(url, savename)
진짜 말 그대로 이게 끝...

--------
## [2]. beautifulsoup라이브러리 활용한 웹페이지 크롤링.
--------
```
from bs4 import Beautiful Soup
html = open("fruits.html", "r", encoding="utf-8") 이렇게 파일을 오픈하고,
soup = BeautifulSoup(html, "html.parser") 로 soup 객채 형성.
```
------
### 1. html 문서의 요소들 정리
띄워쓰기 = 구분이다. (ex – ptag red는 ptag, red가 class에 각각 있는거)    
요소 -> 시작태그 + 종료태그(/)    
태그 -> < >로 둘러싸여 있는거.    
속성 -> 키 = ‘값’    
값 -> 실제 값.    
부모와 자식 관계 -> 안에 들어가있으면 자식, 밖에 자식들을 포함하면 부모.    

------
### 2. bs4 각종 함수들
- 사실 이부분은 개인적인 생각이지만 함수 하나하나 기능과 사용예시까지 자세히 요약하는 것 보다 간단히 정리를 해놓고 필요할때마다 직접 보고 찾아서 그때그때 사용하는 것이 도움것 같다.    
- 따라서 구체적인 사용 예시는 생략하는 대신 함수별로 간단한 요약과 나중에 그 함수를 찾아 사용할때 확인할 페이지 수를 넣었다.
    
전체 함수들 정리– 268 - 269
1. 객체. select_one(<선택자>) - 269 태그 찾기.    
body = soup.select_one("body") - body 태그 찾기     
2. 객체. find(tag，attributes) - 269, 272 조건에 맞는 첫 번째 태그 찾기    
ptag = body.find('p') - body 태그 내에서 1번째 p태그 찾기    
mytag = soup.find(“p", attrs={'class':'hard'})    
3. 새로 속성 추가, 변경 – 270     
ptag['class'][1] = 'white' , ptag[,id']= 'apple‘    
4. 객체. children – 271 for문과 결합하여 그 자식 태그 찾아줌. whitecharacter 문자도 포함하여 출력하니 주의    
for child in body_tag.children:    
5. 객체. find_parent – 272 부모태그 찾아줌.    
print(mydiv.parent)     
6. 객체. attrs ['속성이름'] - find(tag，attributes)에 사용. 272     
7. 객체. find_parents() - 273 모든 상위 태그 출력    
parents = mytag.find_parents()     
8. 객체. findall (tag, attributes, limit=숫자) - 직접적인 등장은 없음. 조건 만족하는 모든 값 판단. 10/03 3번문제에 사용. find와 동일하게 사용하되, 모든 값을 고르고 싶을 때 사용.
--------
### 추가로 조사한 내용
2. 272p 교제에 find함수의 attr를 이용해서 추출하는 코드가 있는데, 이것은 오류가 있는 코드이다. attrs에 해당하는 부분을 { } 로 묶어주어야지만 작동하는 것 주의하자.
   attrs = {' ':' '}꼴.

4. whitecharacter 문자. 찾아봤는데 whitespace character라고 해서 그냥 가로, 혹은 세로로 여백을 주는 문자를 의미한다고 한다. 예를 들어 탭문자, 줄바꿈, 스페이스 등. 이중에서 어느것을 뜻하고 말한건지는 정확하게 알 수 없지만, 태그들 사이에 탭문자, 줄바꿈은 존재하므로 아마 이들 중에서 하나를 뜻하고 말한 것으로 추측한다.
https://zetawiki.com/wiki/%ED%99%94%EC%9D%B4%ED%8A%B8%EC%8A%A4%ED%8E%98%EC%9D%B4%EC%8A%A4
+ 후첨. 직접 fruits.html문서를 만지작 거리면서 확인을 해본 결과 여기서 말한 whitecharacter문자는 줄바꿈문자(개행문자, enter)를 의미하는 것으로 추측된다. 앞의 여백을 의미하는 tap문자를 지웠을때는 출력이 변화가 없었지만, 줄바꿈없이 한줄로 정렬 후 출력하니 앞에 공백이 사라지고, 바로 다음 태그가 출력되는 것을 확인 할 수 있었다. 
+ 그 이유까진 정확히 모르겠으나 여기서 말한 whitecharacter문자는 줄바꿈문자(개행문자, enter)를 의미하는 것이라는 건 확실한 것 같다. 
+ 생각해보니 그냥 여백이 없어서 whitecharacter문자가 사라지므로 없어진 것일수도....    
수정본    
![image](https://user-images.githubusercontent.com/67413252/202885222-11b3aca8-a6f6-447a-bfe5-e937cfe2e102.png)       
결과    
![image](https://user-images.githubusercontent.com/67413252/202885213-ac82abb9-d497-4304-8d4e-b31899be9aff.png)     


---------------
## [3]. CSS 선택자
---------------
### 1. CSS 선택자 종류 
---------------
274
1. '#' id속성, 
2. '.' class속성,
3. '>' child, 
4. '^= &= *=' ~으로 시작하는, 끝나는, 포함하는 속성 찾기
5. 'nth-of-type(su)' - su번째 항목 찾기
---------------
### 2. sellect 함수 
---------------
1. 객체 .select（＜선택자＞）- 275, 277 위의 select_one과 다르게 여러개를 선택할 수 있음.    
findall이랑 비슷하게 사용도 가능하다.     
li.list = soup.seiect(”div#cartoon > ul .elements > li")     
printf(soup. find_alI(“li") [3]. string)     
이런식으로 중첩 사용도 가능     
2. lambda 함수를 통해 바로바로 출력 가능하며, #item5와 같이 상위 태그 상관 없이 id가 item5인걸로 바로 찾을 수도 있다. 276     
choice = 1ambda x : print(soup.seiect_one(x).string)    
choice("#item5")     
3. string – 277 해당 요소의 글자만을 추출함.      
printf(soup. find_alI(“li") [3]. string)     
4. nth-of-type(su)- 278 su번째 항목 찾기, 못찾으면 Nonetype반환     
mystring = mytag.select_one('li:nth-of-type(3)').string     
5. #, . - 278 id, class값 찾기     
print(soup.seiect("#vegatables > li [class='us']")[0].string)    
pri nt(soup.seiect("#vegatables > li.us")[1].string)     
6. ^= &= *= - 279 시작하는, 끝나는, 포함하는 속성 찾기     
result = soup.seiect('a[href$=".com"]')     
for item in result :     
print(item['href'])     
result = soup.select('a[href*="daum"]')    
for item in result :     
print(item['href'])           
7. 메소드 중첩과 정규표현식 – 279             
cond = {"id":"ko"，"class":"cn"}          
pri nt(soup. fi nd (i d="vegatabl es"). fi nd ("1 i", cond) .string)         
li = soup.find.al1(href=re.compi1e(r"Ahttps://"))          
----------------
### 추가로 조사한 내용
find와 select_one, findall과 select의 차이     
역할이 거의 동일한데 왜 굳이 구분지어 놓은것인지 궁금하여 찾아본 결과.     
select종류는 CSS선택자를 통해 구체적인 경로 지정이 가능, find종류는 반복문을 사용해야 한다는 점.     
사실 역할적 측면에서는 큰 차이 없는 듯 하다.     
https://desarraigado.tistory.com/14

----------------
## [4]. 네이버 만화사이트 예제.
너무 길어서 대충 과정만 요약함. 

1. 페이지 소스코드 분석, 자신이 추출할 정보가 어느태그에 속해있는지를 확인.
2. 그 태그에서 자신이 필요한 정보가 아닌 정보들은 제거. replace함수를 사용
3. 자신이 필요한 정보들 각각 분류하기. 붙어있는 정보들은 split으로 분리.
4. 예외(?, :)같은 문자들도 치환.     

이 과정을 계속함. 

-----------------
### 추가로 조사한 내용
287 os.mkdir – 폴더 만드는 함수. 라고 한다. 

-------------------
# 문제 만들기
fruits.html
```html
<html>
    <head>
        <title>제목 없음</title>
    </head>
    <body>
        <p class="ptag red" align="center">사과</p>
        <p class="ptag yellow" align="center">참외</p>
        <p class="ptag_blue" align="center">블루베리</p>
        <div id="container">
            <p class="hard">과일</p>
            <p class="ptag y" align="center">참외1</p>
            <p class="ptag b" align="center">블루베리1</p>
        </div>
    </body>
</html>

```
beautifulsoup 예제03.py
```python
from bs4 import BeautifulSoup

html = open("fruits.html", "r", encoding="utf-8")
soup = BeautifulSoup(html, "html.parser")
body_tag = soup.find('body')
idx = 0
for ch in body_tag.children:
    idx += 1
    print(str(idx)+'번째 요소 :', ch)
```
이 소스코드를 수정하지 않고, 밑의 이미지와 같이 1번째요소부터 4번째 요소까지 연속되게 출력하도록 만들어라.
![image](https://user-images.githubusercontent.com/67413252/202885079-2cdd057b-b64c-4e7d-8d1a-bfc157d93681.png)


