Ch 4. 크롤링
==========
4.1 urllisb 라이브러리를 이용한 웹 페이지 크롤링

-urllib 라이브러리 : 웹 사이트에서 HTML 소스 코드를 읽거나, 데이터 다운로드 등의 작업을 수행할 수 있는 라이브러리
-크롤링 : 웹 페이지의 정보들을  얻어 오는 동작

4.1.1 urllib. request 모듈의 개요

-urllib.request 모듈의 주요 기능 : 
웹을 통하여 데이터를 요청하는 기능, 쿠키를 처리해주는 함수가 있음, 헤더 등 메타데이터를 바꿔주는 함수가 있음
-urlretrieve(url,savename) : 파일 다운

+)쿠키란? : 쿠키는 웹사이트 접속시 접속자의 개인장치에 다운로드 되고 브라우저에 저장되는 작은 텍스트 파일. 웹사이트는 쿠키를 통해 접속자의 장치를 인식하고, 접속자의 설정과 과거 이용내역에 대한 일부 데이터를 저장
+)urlopen(url, data=None, [timeout, ]*, cafile=None, capat=None, cadefault=False, context=None) 
: data는 서버로 전송할 추가 데이터를 지정하는 객체이거나, 그러한 데이터가 필요하지 않으면 none / context
가 지정되면, 다양한 SSL 옵션을 기술는 ssl.SSLContext
 인스턴스이어야 한다. (ex context = ssl._create_unverified_context(),response = urllib.request.urlopenfrequest, context=context))
 

4.1.2 이미지 파일 다운로드 예제
 
import urllib.request # 라이브러리 읽어 들이기

url = "https://shared-comic.pstatic.net/thumb/webtoon/626907/thumbnai1/
title.thumbnail_20150407141027_t83x90.jpg" #url 저장
savename = "urldownloadOl.png"#파일 이름 정하기

urllib.request.urlretrieve（url,savename）
#urlretrieve(url,savename)함수 사용해서 savename이라는 이름의 파일로 다운
print（'웹에 있는 이미지 '+ url + '를 '，end=''）
print（savename + " 파일로 저장하였습니다."）

4.1.3 urlopen() 함수를 이용한 파일 저장

import urllib.request # 라이브러리 읽어 들이기

url = "https://shared-comic.pstatic.net/thumb/webtoon/626907/thumbnai1/
title.thumbnail_20150407141027_t83x90.jpg" #url 저장
savename = "urldownloadOl.png"#파일 이름 정하기

result = urllib.request.urlopen(url)#다운로드

data = result.read()#바이너리 형식으로 변형
print('# type(data) :',type(data))

with open(savename, mode = "wb") as f:
    f,write(data)
    print(savename + "파일로 저장했습니다.")
    
    
4.3  Beautiful Soup 라이브러리를 이용한 웹 페이지 크롤링

4.3.1 HTML 문서 이해하기

fruits.html
<html>
     <head> #헤드 태그
           <title>제목 없음</title>
     </head>
     <body>#바디 태그
          <p class="ptag red" align="center">A|-Il}</p>#class,align 속성
          <p class="ptag yellow" align="center"〉참외</p>#class 속성의 값 ptag, red
          <p c1ass="ptag blue" align="center">블루베리</p>#속성의 값이 2개 이상이면 띄어쓰기로 구분
          <div id="container">
               <p class="hard”>과일</p> #<div>태그의 자식 태그
          </div>
     </body>
</html>

</aside>
----------
from bs4 import Beautiful Soup

html = open("fruits.html", "r", encoding="utf-8")
soup = BeautifulSoup(html, "html.parser")
body = soup.seiect_one("body") #body 태그 추출하기
ptag = body.find('p') #1번쨰 태그 찾기
print('l번째 p태그 : ', ptag['class'])#class 속성의 값 출력

ptag['id']= 'apple' #기존에 없던 id 속성값을 새로 지정
print('l번째 p태그의 id 속성 : ptag['id']) 

body.tag = soup.find('body')
print(body_tag)

ex) mytag = soup.find(”p", attrs='class':'hard') ⇒ p태그 중에서 class=”hard”인 항목을 찾음
    find_parent() 함수 : 상위 태그 (부모태그 찾기)
----------
import re #정규표현 모듈
from bs4 import BeautifulSoup

myencoding = 'utf-8'
myparser = 'html.parser'
filename = 'css01.html'

html = open(filename, encoding=myencoding)
soup = BeautifulSoup(html, myparser)

h1 = soup.select_one("div#cartoon > h1").string #>로 하위 태그를 찾음
print("h1 =", h1)

li_list = soup.select("div#cartoon > ul.elements > li")
for li in li_list:
    print("li =", li.string)

print('-' * 20)

choice = lambda x : print(soup.select_one(x).string)

+)lambda 함수

func = lambda x : x + 1
func(4)

>>>>>5

---------
mytag = soup.select_one('div#cartoon > ul.elements')
mystring = mytag.select_one('li:nth-of-type(3)').string
#nth-of-type(k) : k번째 항목 ,발견되지 않으면 NoneType 반환
print(mystring)
----------
# ^= :  ~으로 시작하는, $= :  ~으로 끝나는
result = soup.select('a[href$=".com"]')
for item in result :
    print(item['href'])

# *= :  daum을 포함하고 있는
result = soup.select('a[href*="daum"]')
for item in result :
    print(item['href'])
-------
4.3.2 Beautiful Soup

<div class="thumb"> 
   <a href="/webtoon/list?titleId=703846&weekday=tue" #타이틀 번호와 요일에 대한 정보
   onclick="nclk_v2（event,'thm*t.img','','1')">
   <imgonerror="this.src='https://ssl .pstatic.net/s...（중략）.. .gif'"
      src="https://shared-comic.pstatic.ne... （중략）... .jpg" width="83" height="90"
      #파일의 경로와 이름
      title="여신강림” alt="여신강링"xspan class="mask"x/span>
      #제목 정보
   </a> 
</div>
<a href="/webtoon/li st.nhn?titleId=703846&weekday=tue"
onclick=Hnclk_v2（event,1 thm*t.tit1,'', '1'）” class=”title” title="여신강림”>여신강림</a>
-----------
#href 속성값을 변수 myhref에 저장하기
myhref = '/webtoon/list?titleid=1234&weekday=mon"

myhref = myhref.replace('/webtoon/1ist?', '')
#실제 분석에 필요가 없어서 replace 함수로 빈 문자열로 치환

result = myhref.split('&')
#타이틀 번호와 요일정보가 기호 &로 나누어져 있어서 &를 기준으로 분해한다

mytitleid = result[0].split('=')[1] #mytitleid[0] : titleid,mytitleid[1] : 1234
myweekday = result[0].split('=')[1] #myweekday[0] : weekday,myweekday[1] : mon
----------
import os
#운영체제에서 제공되는 파일 복사, 폴더 생성, 폴더 내 파일 목록 구하기 등등
#파이썬에서도 가능하게 함
if not os.path.exists(myfolder): # 임시 폴더 생성
        os.mkdir(myfolder)
# 파일 생성os.mkdir(경로)
for mydir in weekday_dict.values() :
        mypath = myfolder + mydir
        if os.path.exists(mypath) :
            pass
        else : # '월요일'부터 '일요일'까지 폴더 생성
            os.mkdir(mypath)
#os.path.exists(file_path) : 존재하면 True, 존재하지 않으면 false 반환
---------------

