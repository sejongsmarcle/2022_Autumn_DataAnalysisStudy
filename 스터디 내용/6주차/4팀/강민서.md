6주차 공부 내용
==========
# 4. 크롤링
크롤러: 웹 페이지의 정보들을 추출하기 위한 프로그램들     
크롤링: 정보를 얻어 오는 동작             


## 4.1. urllib라이브러리를 이용한 웹 페이지 크롤링                
파이썬에서는 urllib라이브러리 제공             


### 4.1.1. urllib.request모듈의 개요
urllib.request 모듈의 주요 기능          
-웹을 통해 데이터 요청 기능          
-쿠키 처리해주는 함수         
-헤더등 메타데이터 바꿔주는 함수           
* 자주 사용되는 함수             
-urlopen(): 네트워크를 통해 원격 객체를 읽고 메모리에 올리는 역할                
-urlretrieve(url, savename): url을 savename이라는 이름의 파일로 다운로드                 


### 4.1.2. urlopen()함수를 이용한 파일 저장            
파이썬의 open()함수를 사용해 파일로 저장 가능              
이미지 파일 다운로드 절차              
1. urlopen 함수를 이용해 이미지 객체 구하기            
2. open함수로 파일 객체(모드: wb)구하기                  
3. 2번 객체.write(1번객체.read())을 이용해 파일 다운로드                      



## 4.3 Beautiful Soup 라이브러리를 이용한 웹 페이지 크롤링
크롤링 대표적인 라이브러리: Beautiful Soup                    
Beautiful Soup: HTML 구문분석 라이브러리, 웹의 태그나 클래스의 값 손쉽게 가져올 수 있도록 지원             
설치 명령어             
-pip install beautifulsoup4               


### 4.3.1. HTML 문서 이해하기
 
* 태그와 속성                
-요소: 시작 태그와 종료 태그의 조합             
-태그: <기호와 >기호로 둘러싸인 범위 안에 명령어 이름 표시            
-속성: 시작 태그 안에 삽입, =기호와 큰따옴표 이용해 값 지정, 태그 내의 [키="값']으로 구성된 항목                 
-값: 속성에 들어가는 실제 값             

* 부모와 자식 태그             
모든 태그는 부모와 자식 또는 형제관계를 맺고 있음                           

* 태그의 속성 다루기                   
HTML의 class 속성과 id 속성에 대해 추가, 조회, 삭제 등의 작업 수행가능                    
-객체.select_one(<선택자>): CSS선택자로 요소 하나 추출                
-객체.find(tag[, attributes]): tag라는 태그 중 조건에 맞는 1번째 태그를 찾기               
-객체.findall(tag, attributes, limit=숫자): tag-찾으려는 tag, attributes-속성으로 이루어진 파이썬의 딕셔너리, 내용-조건에 맞는 HTML 태그를 찾아줌, imit 속성-전체에서 몇 개만 제한하여 추출할 때 사용          
-객체.attrs['속성이름']: 해당 속성들을 딕셔너리 형식으로 보여줌, attrs: attributes줄임말                     
-객체.children: 해당 태그의 하위 태그들을 리스트 목록으로 반환, for문에서 많이 사용                        
-객체.parent: 해당 객체의 부모 요소를 찾아줌                         
-객체.find_parent(): 현재 태그의 바로 위 태그를 찾아줌                      
-객체.find-parents(): 현재 태그의 상위에 았는 모든 태그를 찾아줌, for문을 이용해 추출                   

* CSS 선택자 사용하기                
Beautiful Soup: CSS 선택자를 지정하여 원하는 요소 추출 기능 제공                    
선택자(selector)                      
-#: id속성 의미                      
-.: class속성 의미                                 
->: 현재 대상의 바로 하위child 찾기                        
-^=: 속성의 값이 ~으로 시작하는 값 찾기              
-$=: 속성의 값이 ~으로 끝나는 값 찾기                   
-*=: 속성의 값이 ~을 포함하는 값 찾기                   
-nth-of-type(su): su번째 항목 찾기, one-base indexing 사용                                                  


### 4.3.2. Beautiful Soup                    
일반적으로 웹 사이트에 존재하는 URL에 대해 urlopen()함수와 Beautiful Soup을 조합해 데이터 추출                  

* 크롤링 순서               
<div class="thumb">인 항목 추출                
반복문을 사용해                      
  <a> 태그의 href 속성 읽어 오기                     
      replace()함수로 치환                      
      split()함수를 이용해 요소분해                 
  <img>태그                            
      title 속성을 읽어와서 제목으로 처리                  
      '?'와 ':' 문자는 파일 이름으로 저장 불가능하므로 공백 문자로 치환               
      src 속성을 읽어와서 이미지가 존재하는 경로 취득                                         
      
  필요한 정보를 리스트에 추가                    
  해당 이미지를 각 요일 폴더에 이미지로 저장                                           
  
데이터프레임으로 만들어 CSV 파일로 저장                 

# 5. 데이터 수집, 전처리, 시각화                       


## 5.1. 프로젝트: 치킨 회사 홈페이지에서 매장 위치 정보 수집하기                                       

### 5.1.1. 모듈 공통사항              
*구현할 내용: 나는 페리카나로 선정                    

크롤링된 파일들은 '브랜드.csv'형식으로 파일 저장                                           
칼럼 정보: 브랜드 이름(brand), 시도(sido), 군구(gungu), 주소지(address), 전화 번호(phone)                       
* 공용 모듈                   
    생성자: brandName, url, soup, driver                   
    get_request_url(): 응답객체 구해주는 함수                  
    getSoup(): Beautiful Soup객체 반환 함수                       
    save2Csv(): 데이터프레임 액셀 파일로 저장                          
                                  

### 5.1.2. 페리카나 치킨의 매장 정보 크롤링                  
    페이지 번호는 count() 함수로 사용                                               
    
* 카운터 프로그램(itertools)                      
count() 함수를 사용해 마지막 페이지에 도달하면 프로그램 강제 종료                

* 페리카나 치킨 매장 정보                     
    치킨 매장 찾기 웹페이지로 이동, page 파라미터를 제외한 나머지 파라미터는 신경X                
    page 파라미터는 수시로 변경                    
    
* 크롤링 코딩 순서                
    -테이블 외관을 구성하는 <table class="table mt20"인 항목 찾기                               
    -하위 태그 중에서 <tbody>찾기              
    -<tbody>태그 내에서 모든 <tr>태그 찾기              
    -<tr>태그 1개는 한개의 매장 정보 의미              
    -shopExists는 마지막 페이지인지를 판별하기 위한 변수             
    -마지막 페이지에 도달하면 shopExists = True                       
    -개발자의 실수인지 중간에 매장 주소 정보가 없는 경우, if문으로 분리 처리                                                              
    

	  
	  
    
* 새롭게 알게 된 내용                  
------
-beautiful soup의 html.parser는 시간을 많이 잡아 먹는다              
-슬라이싱을 이용하면 parser부분에서 부하가 줄어든다.                   
-byte.index(b'태그')하면 빠르게 index값을 받을 수 있다.                                                     
	  

 
	  
	  
* 내가 만든 문제                                   
----              
div 태그 중에서 id속성의 값이 main-goods인 항목을 찾고, 하위의 h1태그의 문자열을 출력하라. 그리고 'vegtables'항목을 출력하라                                    
```python
<html>
<head></head>
<body>
	<div id="main-goods">
		<h1>야채와 과일</h1>
		<ul id="vegetables">
			<li>당근</li>
			<li>호박</li>
			<li>양파</li>
			<li>가지</li>
		</ul>
		<ul id="fruits">
			<li>사과</li>
			<li class="us">감</li>
			<li class="us">밤</li>
			<li class="black" data-lo="us">대추</li>
			<li class="cn" id="ko">배</li>
		</ul>
	</div>
</body>
</html>
```
	  
******
	
    
* 출력 예시)          
    h1 = 야채와 과일                
    li = 당근            
    li = 호박              
    li = 양파              
    li = 가지                    
	  
	  
*****
	  
* 정답
	  
	  
```python
import re
from bs4 import BeautifulSoup

myencoding = 'utf-8'
myparser = 'html.parser'
filename = 'css01.html'

html = open(filename, encoding=myencoding)
soup = BeautifulSoup(html, myparser)
#.string은 해당요소의 글자 부분 추출
h1 = soup.select_one("div#main-goods > h1").string #>현재 대상의 바로 하위 child
print("h1 =", h1)
#eselect()함수는 요소 여러개를 리스트 형식으로 반환, >기호는 중첩 가능
#선택자 class는 . id는 #
li_list = soup.select('div#main-goods > ul#fruits > li')
for li in li_list:
    print("li =", li.string)
```
    
