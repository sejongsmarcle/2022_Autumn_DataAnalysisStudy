6주차 공부 내용
==========
# 4. 크롤링
크롤러: 웹 페이지의 정보들을 추출하기 위한 프로그램들
크롤링: 정보를 얻어 오는 동작

## 4.1. urllib라이브러리를 이용한 웹 페이지 크롤링
파이썬에서는 urllib라이브러리 제공

### 4.1.1. urllib.request모듈의 개요
urllib.request 모듈의 주요 기능
-웹을 통해 데이터 요청 기능
-쿠키 처리해주는 함수
-헤더등 메타데이터 바꿔주는 함수
*자주 사용되는 함수
-urlopen(): 네트워크를 통해 원격 객체를 읽고 메모리에 올리는 역할
-urlretrieve(url, savename): url을 savename이라는 이름의 파일로 다운로드

### 4.1.2. urlopen()함수를 이용한 파일 저장
파이썬의 open()함수를 사용해 파일로 저장 가능
이미지 파일 다운로드 절차
1. urlopen 함수를 이용해 이미지 객체 구하기
2. open함수로 파일 객체(모드: wb)구하기
3. 2번 객체.write(1번객체.read())을 이용해 파일 다운로드

## 4.3 Beautiful Soup 라이브러리를 이용한 웹 페이지 크롤링
크롤링 대표적인 라이브러리: Beautiful Soup 
Beautiful Soup: HTML 구문분석 라이브러리, 웹의 태그나 클래스의 값 손쉽게 가져올 수 있도록 지원
설치 명령어
-pip install beautifulsoup4

### 4.3.1. HTML 문서 이해하기
*태그와 속성
-요소: 시작 태그와 종료 태그의 조합
-태그: <기호와 >기호로 둘러싸인 범위 안에 명령어 이름 표시
-속성: 시작 태그 안에 삽입, =기호와 큰따옴표 이용해 값 지정, 태그 내의 [키="값']으로 구성된 항목
-값: 속성에 들어가는 실제 값

*부모와 자식 태그
모든 태그는 부모와 자식 또는 형제관계를 맺고 있음

*태그의 속성 다루기
HTML의 class 속성과 id 속성에 대해 추가, 조회, 삭제 등의 작업 수행가능
-객체.select_one(<선택자>): CSS선택자로 요소 하나 추출
-객체.find(tag[, attributes]): tag라는 태그 중 조건에 맞는 1번째 태그를 찾기
-객체.findall(tag, attributes, limit=숫자): tag-찾으려는 tag, attributes-속성으로 이루어진 파이썬의 딕셔너리, 내용-조건에 맞는 HTML 태그를 찾아줌, imit 속성-전체에서 몇 개만 제한하여 추출할 때 사용
-객체.attrs['속성이름']: 해당 속성들을 딕셔너리 형식으로 보여줌, attrs: attributes줄임말
-객체.children: 해당 태그의 하위 태그들을 리스트 목록으로 반환, for문에서 많이 사용
-객체.parent: 해당 객체의 부모 요소를 찾아줌
-객체.find_parent(): 현재 태그의 바로 위 태그를 찾아줌
-객체.find-parents(): 현재 태그의 상위에 았는 모든 태그를 찾아줌, for문을 이용해 추출

*CSS 선택자 사용하기
Beautiful Soup: CSS 선택자를 지정하여 원하는 요소 추출 기능 제공
선택자(selector)
-#: id속성 의미
-.: class속성 의미
->: 현재 대상의 바로 하위child 찾기
-^=: 속성의 값이 ~으로 시작하는 값 찾기
-$=: 속성의 값이 ~으로 끝나는 값 찾기
-*=: 속성의 값이 ~을 포함하는 값 찾기
-nth-of-type(su): su번째 항목 찾기, one-base indexing 사용

### 4.3.2. Beautiful Soup
일반적으로 웹 사이트에 존재하는 URL에 대해 urlopen()함수와 Beautiful Soup을 조합해 데이터 추출

*크롤링 순서
<div class="thumb">인 항목 추출
반복문을 사용해
  <a> 태그의 href 속성 읽어 오기
      replace()함수로 치환
      split()함수를 이용해 요소분해
  <img>태그
      title 속성을 읽어와서 제목으로 처리
      '?'와 ':' 문자는 파일 이름으로 저장 불가능하므로 공백 문자로 치환
      src 속성을 읽어와서 이미지가 존재하는 경로 취득
      
  필요한 정보를 리스트에 추가
  해당 이미지를 각 요일 폴더에 이미지로 저장
  
데이터프레임으로 만들어 CSV 파일로 저장 
